# üí¨ An√°lise de Sentimentos em Texto (NLP)

## üí° Vis√£o Geral do Projeto

Este projeto demonstra a aplica√ß√£o de **Processamento de Linguagem Natural (NLP)** para classificar o sentimento expresso em textos (como coment√°rios e feedback de clientes) nas categorias **Positivo, Negativo e Neutro**. O objetivo √© comparar a efic√°cia de modelos de Machine Learning Cl√°ssico vs. Deep Learning na compreens√£o do contexto e da sem√¢ntica da linguagem.

O resultado √© uma ferramenta cr√≠tica para a **An√°lise de Dados de Clientes** e a **Monitoriza√ß√£o de Marca**.

---

## ‚öôÔ∏è Metodologia e Arquitetura

O projeto foi dividido em duas fases para ilustrar a evolu√ß√£o das t√©cnicas de NLP:

### 1. Fase Cl√°ssica (Linha de Base)

| Componente | T√©cnica | Objetivo |
| :--- | :--- | :--- |
| **Pr√©-processamento** | CountVectorizer (Bag-of-Words) | Converte texto em vetores de frequ√™ncia de palavras. |
| **Modelo** | Multinomial Naive Bayes | Classificador probabil√≠stico simples e eficiente para vetores esparsos. |
| **Resultado** | Baixa precis√£o em dados limitados | Demonstra a fragilidade de BoW/Naive Bayes ao lidar com sem√¢ntica e contexto. |

### 2. Fase Deep Learning (Solu√ß√£o Final)

A arquitetura de Deep Learning √© essencial para capturar o contexto temporal e a sem√¢ntica de frases complexas.

| Camada | Fun√ß√£o no Processamento de Linguagem |  |
| :--- | :--- | :--- |
| **Tokeniza√ß√£o/Padding** | Converte palavras em sequ√™ncias num√©ricas de tamanho fixo. | |
| **Embedding Layer** | Mapeia palavras para vetores densos (embeddings), capturando o significado real (sem√¢ntica). | |
| **LSTM (RNN)** | Camada Recorrente que entende a **ordem** e o **contexto** das palavras (ex: nega√ß√£o). | |
| **Camada Densa (Sa√≠da)** | Mapeia o contexto aprendido pela LSTM para a classifica√ß√£o final (Positivo/Negativo/Neutro). | |

---

## ‚úÖ Resultados e Conclus√£o

| Previs√£o | Modelo Cl√°ssico (Naive Bayes) | Modelo Deep Learning (LSTM) |
| :--- | :--- | :--- |
| **Coment√°rio de Teste** | "O produto √© incr√≠vel, superou todas as expectativas!" | "O produto √© incr√≠vel, superou todas as expectativas!" |
| **Sentimento Previsto** | **Neutro** (Incorreto) | **Positivo** (Correto) |

A transi√ß√£o para a arquitetura LSTM resolveu o problema de classifica√ß√£o, provando que o **Deep Learning √© superior para tarefas de compreens√£o textual (NLP)**, pois √© capaz de inferir a inten√ß√£o real ("incr√≠vel", "superou") mesmo com *datasets* pequenos, devido √† sua capacidade de processar a sequ√™ncia do texto.

---
